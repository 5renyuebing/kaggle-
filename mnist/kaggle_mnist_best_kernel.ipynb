{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##数据处理\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "dataset=pd.read_csv(\"C:\\\\Users\\\\LCL\\\\Desktop\\\\sklearn_tensorflow\\\\kaggle\\\\Projects\\\\mnist\\\\train.csv\")\n",
    "testset=pd.read_csv(\"C:\\\\Users\\\\LCL\\\\Desktop\\\\sklearn_tensorflow\\\\kaggle\\\\Projects\\\\mnist\\\\test.csv\")\n",
    "\n",
    "#分层采样\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "for train_index, test_index in split.split(dataset, dataset[\"label\"]):\n",
    "    train = dataset.loc[train_index]\n",
    "    val = dataset.loc[test_index]\n",
    "X_train=train.drop(\"label\", axis=1)\n",
    "y_train=train[\"label\"].copy()\n",
    "X_val=val.drop(\"label\", axis=1)\n",
    "y_val=val[\"label\"].copy()\n",
    "\n",
    "#检验分层采样结果\n",
    "y_train.value_counts()/y_train.shape[0]\n",
    "y_val.value_counts()/y_val.shape[0]\n",
    "\n",
    "#清理多余内容\n",
    "del train\n",
    "del val\n",
    "\n",
    "#检查是否有缺失值\n",
    "X_train.isnull().any().describe()\n",
    "testset.isnull().any().describe()\n",
    "\n",
    "#标准化\n",
    "X_train = X_train / 255.0\n",
    "testset = testset / 255.0\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "y_train=y_train.values\n",
    "X_val = X_val.values.reshape(-1,28,28,1)\n",
    "y_val=y_val.values\n",
    "X_test = testset.values.reshape(-1,28,28,1)\n",
    "\n",
    "#数据增强\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)\n",
    "X_train_list=[]\n",
    "y_train_list=[]\n",
    "for i in range(X_train.shape[0]):\n",
    "    gen=datagen.flow(X_train[i].reshape(1,28,28,1),batch_size=1)\n",
    "    X_train_list.append(X_train[i].reshape(1,28,28,1))\n",
    "    y_train_list.append(y_train[i])\n",
    "    for j in range(5):\n",
    "        x_batch = next(gen)\n",
    "        X_train_list.append(x_batch)\n",
    "        y_train_list.append(y_train[i])\n",
    "X_train=np.array(X_train_list)\n",
    "X_train=X_train.reshape(X_train.shape[0],28,28,1)\n",
    "y_train=np.array(y_train_list)\n",
    "        \n",
    "\n",
    "#检查结果\n",
    "# import matplotlib.pyplot as plt\n",
    "# import time\n",
    "# for i in range(50):\n",
    "#     plt.imshow(X_train[(i*10+5)*7][:,:,0])\n",
    "#     plt.show()\n",
    "#     print(y_train[(i*10+5)*7])\n",
    "#     time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CNN\n",
    "from functools import partial\n",
    "\n",
    "X_TEST_SHAPE=28000\n",
    "INPUT_SHAPE=28\n",
    "INPUT_CHANNEL=1\n",
    "KERNEL_SIZE_1=5\n",
    "FILTERS_1=32\n",
    "KERNEL_SIZE_2=3\n",
    "FILTERS_2=64\n",
    "DROPOUT_CONV=0.25\n",
    "DROPOUT_DENSE=0.5\n",
    "N_DENSE=256\n",
    "N_OUTPUTS=10\n",
    "LEARNING_RATE=0.001\n",
    "BATCH_SIZE=200\n",
    "BATCH_SIZE_TEST=7000\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "my_pool_layer=partial(tf.nn.max_pool,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"VALID\")\n",
    "\n",
    "X=tf.placeholder(tf.float32,shape=(BATCH_SIZE,INPUT_SHAPE,INPUT_SHAPE,1),name=\"X\")\n",
    "X_valid=tf.placeholder(tf.float32,shape=(X_val.shape[0],INPUT_SHAPE,INPUT_SHAPE,1),name=\"X_valid\")\n",
    "X_test_input=tf.placeholder(tf.float32,shape=(BATCH_SIZE_TEST,INPUT_SHAPE,INPUT_SHAPE,1),name=\"X_test\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "def conv(X,name,input_channel,kernel_size,filters):\n",
    "    with tf.variable_scope(name):\n",
    "        he_init = tf.variance_scaling_initializer()\n",
    "        kernel_filters=tf.get_variable(initializer=he_init,shape=(kernel_size,kernel_size,input_channel,filters),\n",
    "                                name=\"kernel\")\n",
    "        bias=tf.get_variable(initializer=tf.zeros([filters]),name=\"bias\")\n",
    "        convolution=tf.nn.conv2d(X,kernel_filters,strides=[1,1,1,1],padding=\"SAME\")\n",
    "        relu=tf.nn.relu(tf.nn.bias_add(convolution,bias))\n",
    "        return relu\n",
    "\n",
    "#用于训练\n",
    "with tf.variable_scope(\"convolution_layer\"):\n",
    "    \n",
    "    conv1=conv(X,\"conv1\",INPUT_CHANNEL,KERNEL_SIZE_1,FILTERS_1)\n",
    "    conv2=conv(conv1,\"conv2\",FILTERS_1,KERNEL_SIZE_1,FILTERS_1)\n",
    "    pool1=my_pool_layer(conv2,name=\"pool1\")\n",
    "    pool1_drop=tf.layers.dropout(pool1,DROPOUT_CONV,training=training)\n",
    "    \n",
    "    conv3=conv(pool1,\"conv3\",FILTERS_1,KERNEL_SIZE_2,FILTERS_2)\n",
    "    conv4=conv(conv3,\"conv4\",FILTERS_2,KERNEL_SIZE_2,FILTERS_2)\n",
    "    pool2=my_pool_layer(conv4,name=\"pool2\")\n",
    "    pool2_drop=tf.layers.dropout(pool2,DROPOUT_CONV,training=training)\n",
    "\n",
    "#用于验证\n",
    "with tf.variable_scope(\"convolution_layer\",reuse=True):\n",
    "    \n",
    "    conv1_valid=conv(X_valid,\"conv1\",INPUT_CHANNEL,KERNEL_SIZE_1,FILTERS_1)\n",
    "    conv2_valid=conv(conv1_valid,\"conv2\",FILTERS_1,KERNEL_SIZE_1,FILTERS_1)\n",
    "    pool1_valid=my_pool_layer(conv2_valid,name=\"pool1\")\n",
    "    pool1_drop_valid=tf.layers.dropout(pool1_valid,DROPOUT_CONV,training=training)\n",
    "    \n",
    "    conv3_valid=conv(pool1_valid,\"conv3\",FILTERS_1,KERNEL_SIZE_2,FILTERS_2)\n",
    "    conv4_valid=conv(conv3_valid,\"conv4\",FILTERS_2,KERNEL_SIZE_2,FILTERS_2)\n",
    "    pool2_valid=my_pool_layer(conv4_valid,name=\"pool2\")\n",
    "    pool2_drop_valid=tf.layers.dropout(pool2_valid,DROPOUT_CONV,training=training)\n",
    "    \n",
    "#用于测试\n",
    "with tf.variable_scope(\"convolution_layer\",reuse=True):\n",
    "    \n",
    "    conv1_test=conv(X_test_input,\"conv1\",INPUT_CHANNEL,KERNEL_SIZE_1,FILTERS_1)\n",
    "    conv2_test=conv(conv1_test,\"conv2\",FILTERS_1,KERNEL_SIZE_1,FILTERS_1)\n",
    "    pool1_test=my_pool_layer(conv2_test,name=\"pool1\")\n",
    "    pool1_drop_test=tf.layers.dropout(pool1_test,DROPOUT_CONV,training=training)\n",
    "    \n",
    "    conv3_test=conv(pool1_test,\"conv3\",FILTERS_1,KERNEL_SIZE_2,FILTERS_2)\n",
    "    conv4_test=conv(conv3_test,\"conv4\",FILTERS_2,KERNEL_SIZE_2,FILTERS_2)\n",
    "    pool2_test=my_pool_layer(conv4_test,name=\"pool2\")\n",
    "    pool2_drop_test=tf.layers.dropout(pool2_test,DROPOUT_CONV,training=training)\n",
    "    \n",
    "def neuron_layer(X, n_neurons, name, init,activation=None):\n",
    "    with tf.variable_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        W = tf.get_variable(initializer=init,shape=(n_inputs,n_neurons),name=\"kernel\")\n",
    "        b = tf.get_variable(initializer=tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z\n",
    "\n",
    "#用于训练\n",
    "with tf.variable_scope(\"full_connected_layer\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "    flatten=tf.layers.flatten(pool2_drop)\n",
    "    dense = neuron_layer(flatten,N_DENSE,name=\"dense\",init=he_init,activation=tf.nn.relu)\n",
    "    dense_drop=tf.layers.dropout(dense,DROPOUT_DENSE,training=training)\n",
    "    logits = neuron_layer(dense_drop,N_OUTPUTS,name=\"outputs\",init=he_init)\n",
    "\n",
    "#用于验证\n",
    "with tf.variable_scope(\"full_connected_layer\",reuse=True):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "    flatten_valid=tf.layers.flatten(pool2_drop_valid)\n",
    "    dense_valid = neuron_layer(flatten_valid,N_DENSE,name=\"dense\",init=he_init,activation=tf.nn.relu)\n",
    "    dense_drop_valid=tf.layers.dropout(dense_valid,DROPOUT_DENSE,training=training)\n",
    "    logits_valid = neuron_layer(dense_drop_valid,N_OUTPUTS,name=\"outputs\",init=he_init)\n",
    "\n",
    "#用于测试\n",
    "with tf.variable_scope(\"full_connected_layer\",reuse=True):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "    flatten_test=tf.layers.flatten(pool2_drop_test)\n",
    "    dense_test = neuron_layer(flatten_test,N_DENSE,name=\"dense\",init=he_init,activation=tf.nn.relu)\n",
    "    dense_drop_test=tf.layers.dropout(dense_test,DROPOUT_DENSE,training=training)\n",
    "    logits_test = neuron_layer(dense_drop_test,N_OUTPUTS,name=\"outputs\",init=he_init)\n",
    "\n",
    "#加入collection\n",
    "tf.add_to_collection(\"collection\",logits_test)\n",
    "\n",
    "#用于训练\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32),name=\"accuracy\")\n",
    "    \n",
    "#用于验证\n",
    "with tf.name_scope(\"eval_test\"):\n",
    "    correct_valid = tf.nn.in_top_k(logits_valid, y, 1)\n",
    "    accuracy_valid = tf.reduce_mean(tf.cast(correct_valid, tf.float32),name=\"accuracy\")\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  valid accurcay: 0.990238\n",
      "epoch: 1  valid accurcay: 0.992857\n",
      "epoch: 2  valid accurcay: 0.994048\n",
      "epoch: 3  valid accurcay: 0.993095\n",
      "epoch: 4  valid accurcay: 0.994048\n",
      "epoch: 5  valid accurcay: 0.995\n",
      "epoch: 6  valid accurcay: 0.994762\n",
      "epoch: 7  valid accurcay: 0.994762\n",
      "epoch: 8  valid accurcay: 0.994762\n",
      "epoch: 9  valid accurcay: 0.995\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, BATCH_SIZE):\n",
    "            sess.run(training_op,\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "            \n",
    "#使用验证集检验当前模型准确率\n",
    "        accuracy=sess.run(accuracy_valid,feed_dict={X_valid:X_val,y:y_val})\n",
    "        print(\"epoch:\",epoch,\" valid accurcay:\",accuracy)\n",
    "        \n",
    "#对测试集分批，否则会产生OOM\n",
    "    y_pred_list=[]\n",
    "    for i in range(int(X_TEST_SHAPE/BATCH_SIZE_TEST)):\n",
    "        Z = logits_test.eval(feed_dict={X_test_input:X_test[i*BATCH_SIZE_TEST:i*BATCH_SIZE_TEST+BATCH_SIZE_TEST]})    \n",
    "        y_pred = np.argmax(Z, axis=1)\n",
    "        y_pred_list.append(y_pred)\n",
    "        \n",
    "    save_path = saver.save(sess, \"C:\\\\Users\\\\LCL\\\\Desktop\\\\sklearn_tensorflow\\\\kaggle\\\\Projects\\\\mnist\\\\mnist_model.ckpt\")\n",
    "\n",
    "y_pred=y_pred_list[0]\n",
    "for pred in y_pred_list[1:]:\n",
    "    y_pred=np.append(y_pred,pred)\n",
    "\n",
    "#保存为csv文件\n",
    "y_df=pd.DataFrame(y_pred)\n",
    "y_df.to_csv('C:\\\\Users\\\\LCL\\\\Desktop\\\\sklearn_tensorflow\\\\kaggle\\\\Projects\\\\mnist\\\\data_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
